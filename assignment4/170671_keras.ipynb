{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "def loadIrisData():\n",
    "    iris = load_iris()\n",
    "    X=iris['data']\n",
    "    t=iris['target']\n",
    "    print(X.shape)\n",
    "    print(t.shape)\n",
    "    return X, t\n",
    "X, t = loadIrisData()\n",
    "# np.insert(y,0,1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(t_indices, N):\n",
    "    '''\n",
    "    Inputs:\n",
    "        t_indices: list of indices\n",
    "        N: total no. of classes\n",
    "    '''\n",
    "    assert N>max(t_indices), (N, max(t_indices))\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "    one_hot_encoded = np.zeros((len(t_indices), N))\n",
    "    unique_list = np.unique(t_indices)\n",
    "    \n",
    "    for i in unique_list:\n",
    "        idx = np.where(t_indices==i)[0]\n",
    "#         print(idx)\n",
    "        one_hot_encoded[idx, i] = 1\n",
    "\n",
    "    t_1hot = one_hot_encoded\n",
    "    return t_1hot\n",
    "t = one_hot_encoding(t, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X,t,testFraction=0.2):\n",
    "    \"\"\"\n",
    "    Use numpy functions only\n",
    "    Inputs:\n",
    "        X: np array of shape (Nsamples, dim)\n",
    "        t: np array of len Nsamples; can be one hot vectors or labels\n",
    "        testFraction: (float) Nsamples_test = testFraction * Nsamples\n",
    "    \"\"\"\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "#     print(np.floor(len(X)*testFraction))\n",
    "    arr = np.arange(len(X))\n",
    "    np.random.shuffle(arr)\n",
    "    X = X[arr]\n",
    "    t = t[arr]\n",
    "    \n",
    "    X_test = X[:int(np.floor(testFraction*len(X))), :]\n",
    "    t_test = t[:int(np.floor(testFraction*len(t))), :]\n",
    "    \n",
    "    X_train = X[int(np.floor(testFraction*len(X))):, :]\n",
    "    t_train = t[int(np.floor(testFraction*len(t))):, :]\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "X_train, t_train, X_test, t_test = splitData(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize data to be of zero mean and unit variance\n",
    "def normalizeX(X_train, X_test):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X_train: np array 2d\n",
    "        X_test: np array 2d\n",
    "    Outputs:\n",
    "        Normalized np arrays 2d\n",
    "    '''\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "    \n",
    "    X_test_normalized = (X_test-np.float32([np.mean(X_test, axis=0)]))\n",
    "    X_test_normalized = X_test_normalized/np.float32([np.std(X_test_normalized, axis=0)])\n",
    "    X_train_normalized = (X_train-np.float32([np.mean(X_train, axis=0)]))\n",
    "    X_train_normalized = X_train_normalized/np.float32([np.std(X_train_normalized, axis=0)])\n",
    "\n",
    "    return X_train_normalized, X_test_normalized\n",
    "\n",
    "X_train, X_test = normalizeX(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s = tf.keras.Sequential()\n",
    "model_s.add(tf.keras.layers.Dense(10, input_dim=4, activation=tf.keras.layers.Activation('sigmoid')))\n",
    "model_s.add(tf.keras.layers.Dense(3, activation=tf.keras.layers.Activation('sigmoid')))\n",
    "\n",
    "tensorboard_callback_s = tf.keras.callbacks.TensorBoard(log_dir='./logdir_sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s.compile(optimizer=tf.optimizers.RMSprop(),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0923 13:56:29.928368 139682939295552 summary_ops_v2.py:1110] Model failed to serialize as JSON. Ignoring... 'Activation' object has no attribute '__name__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 1s 6ms/sample - loss: 0.2565 - accuracy: 0.3417 - val_loss: 0.2574 - val_accuracy: 0.3000\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.2513 - accuracy: 0.3417 - val_loss: 0.2533 - val_accuracy: 0.3000\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.2478 - accuracy: 0.3417 - val_loss: 0.2500 - val_accuracy: 0.3000\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.2446 - accuracy: 0.3417 - val_loss: 0.2470 - val_accuracy: 0.3000\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.2419 - accuracy: 0.3417 - val_loss: 0.2442 - val_accuracy: 0.3000\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 0.2392 - accuracy: 0.3417 - val_loss: 0.2416 - val_accuracy: 0.3000\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.2367 - accuracy: 0.3417 - val_loss: 0.2390 - val_accuracy: 0.3000\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.2343 - accuracy: 0.3417 - val_loss: 0.2365 - val_accuracy: 0.3000\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.2319 - accuracy: 0.3417 - val_loss: 0.2341 - val_accuracy: 0.3000\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 104us/sample - loss: 0.2297 - accuracy: 0.3417 - val_loss: 0.2317 - val_accuracy: 0.3000\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.2274 - accuracy: 0.3417 - val_loss: 0.2294 - val_accuracy: 0.3000\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 97us/sample - loss: 0.2252 - accuracy: 0.3417 - val_loss: 0.2271 - val_accuracy: 0.3000\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 76us/sample - loss: 0.2231 - accuracy: 0.3417 - val_loss: 0.2249 - val_accuracy: 0.3000\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.2211 - accuracy: 0.3417 - val_loss: 0.2227 - val_accuracy: 0.3000\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.2190 - accuracy: 0.3417 - val_loss: 0.2206 - val_accuracy: 0.3000\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.2171 - accuracy: 0.3417 - val_loss: 0.2186 - val_accuracy: 0.3000\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 104us/sample - loss: 0.2151 - accuracy: 0.3500 - val_loss: 0.2165 - val_accuracy: 0.3000\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.2133 - accuracy: 0.3500 - val_loss: 0.2146 - val_accuracy: 0.3000\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 102us/sample - loss: 0.2114 - accuracy: 0.3667 - val_loss: 0.2127 - val_accuracy: 0.3333\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 83us/sample - loss: 0.2097 - accuracy: 0.4000 - val_loss: 0.2109 - val_accuracy: 0.3667\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.2079 - accuracy: 0.4167 - val_loss: 0.2090 - val_accuracy: 0.4333\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.2063 - accuracy: 0.4833 - val_loss: 0.2072 - val_accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.2046 - accuracy: 0.5167 - val_loss: 0.2055 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 68us/sample - loss: 0.2030 - accuracy: 0.5750 - val_loss: 0.2038 - val_accuracy: 0.5333\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.2015 - accuracy: 0.5917 - val_loss: 0.2021 - val_accuracy: 0.6333\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.1999 - accuracy: 0.6000 - val_loss: 0.2005 - val_accuracy: 0.6333\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.1984 - accuracy: 0.6417 - val_loss: 0.1990 - val_accuracy: 0.6667\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.1970 - accuracy: 0.6667 - val_loss: 0.1974 - val_accuracy: 0.6667\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.1957 - accuracy: 0.6833 - val_loss: 0.1961 - val_accuracy: 0.6667\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.1942 - accuracy: 0.6833 - val_loss: 0.1946 - val_accuracy: 0.6667\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.1928 - accuracy: 0.6917 - val_loss: 0.1932 - val_accuracy: 0.6667\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 96us/sample - loss: 0.1915 - accuracy: 0.7000 - val_loss: 0.1917 - val_accuracy: 0.6667\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.1901 - accuracy: 0.7167 - val_loss: 0.1903 - val_accuracy: 0.6667\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.1887 - accuracy: 0.7167 - val_loss: 0.1889 - val_accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 107us/sample - loss: 0.1874 - accuracy: 0.7250 - val_loss: 0.1875 - val_accuracy: 0.6667\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 104us/sample - loss: 0.1861 - accuracy: 0.7250 - val_loss: 0.1861 - val_accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 87us/sample - loss: 0.1848 - accuracy: 0.7333 - val_loss: 0.1847 - val_accuracy: 0.7333\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 104us/sample - loss: 0.1836 - accuracy: 0.7333 - val_loss: 0.1834 - val_accuracy: 0.7333\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.1823 - accuracy: 0.7333 - val_loss: 0.1822 - val_accuracy: 0.7333\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.1811 - accuracy: 0.7333 - val_loss: 0.1808 - val_accuracy: 0.7667\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.1798 - accuracy: 0.7417 - val_loss: 0.1795 - val_accuracy: 0.7667\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.1786 - accuracy: 0.7500 - val_loss: 0.1782 - val_accuracy: 0.7667\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 104us/sample - loss: 0.1774 - accuracy: 0.7500 - val_loss: 0.1770 - val_accuracy: 0.7667\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.1762 - accuracy: 0.7583 - val_loss: 0.1757 - val_accuracy: 0.7667\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.1749 - accuracy: 0.7583 - val_loss: 0.1745 - val_accuracy: 0.7667\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 120us/sample - loss: 0.1738 - accuracy: 0.7667 - val_loss: 0.1733 - val_accuracy: 0.7667\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 97us/sample - loss: 0.1726 - accuracy: 0.7667 - val_loss: 0.1721 - val_accuracy: 0.7667\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.1715 - accuracy: 0.7667 - val_loss: 0.1709 - val_accuracy: 0.7667\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.1704 - accuracy: 0.7750 - val_loss: 0.1697 - val_accuracy: 0.7667\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.1691 - accuracy: 0.7750 - val_loss: 0.1685 - val_accuracy: 0.7667\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 102us/sample - loss: 0.1680 - accuracy: 0.7750 - val_loss: 0.1673 - val_accuracy: 0.7667\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 127us/sample - loss: 0.1669 - accuracy: 0.7833 - val_loss: 0.1661 - val_accuracy: 0.8000\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 127us/sample - loss: 0.1658 - accuracy: 0.7833 - val_loss: 0.1650 - val_accuracy: 0.8000\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 114us/sample - loss: 0.1646 - accuracy: 0.7833 - val_loss: 0.1638 - val_accuracy: 0.8000\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 89us/sample - loss: 0.1635 - accuracy: 0.7833 - val_loss: 0.1627 - val_accuracy: 0.8000\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.1625 - accuracy: 0.7833 - val_loss: 0.1616 - val_accuracy: 0.8000\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 126us/sample - loss: 0.1613 - accuracy: 0.7833 - val_loss: 0.1605 - val_accuracy: 0.8000\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.1602 - accuracy: 0.7833 - val_loss: 0.1594 - val_accuracy: 0.8000\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 114us/sample - loss: 0.1592 - accuracy: 0.7833 - val_loss: 0.1583 - val_accuracy: 0.8000\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.1581 - accuracy: 0.7833 - val_loss: 0.1572 - val_accuracy: 0.8333\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.1571 - accuracy: 0.7833 - val_loss: 0.1561 - val_accuracy: 0.8333\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 148us/sample - loss: 0.1560 - accuracy: 0.7833 - val_loss: 0.1551 - val_accuracy: 0.8333\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.1550 - accuracy: 0.7833 - val_loss: 0.1541 - val_accuracy: 0.8333\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 126us/sample - loss: 0.1540 - accuracy: 0.7833 - val_loss: 0.1531 - val_accuracy: 0.8333\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 160us/sample - loss: 0.1530 - accuracy: 0.7917 - val_loss: 0.1520 - val_accuracy: 0.8333\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.1520 - accuracy: 0.7917 - val_loss: 0.1510 - val_accuracy: 0.8333\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.1509 - accuracy: 0.8000 - val_loss: 0.1500 - val_accuracy: 0.8333\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.1499 - accuracy: 0.7917 - val_loss: 0.1490 - val_accuracy: 0.8333\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 0.1489 - accuracy: 0.7917 - val_loss: 0.1479 - val_accuracy: 0.8333\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.1481 - accuracy: 0.8000 - val_loss: 0.1470 - val_accuracy: 0.8333\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 130us/sample - loss: 0.1472 - accuracy: 0.8000 - val_loss: 0.1461 - val_accuracy: 0.8333\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.1461 - accuracy: 0.8000 - val_loss: 0.1452 - val_accuracy: 0.8333\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 112us/sample - loss: 0.1452 - accuracy: 0.7917 - val_loss: 0.1443 - val_accuracy: 0.8333\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 119us/sample - loss: 0.1443 - accuracy: 0.8000 - val_loss: 0.1434 - val_accuracy: 0.8333\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.1434 - accuracy: 0.8000 - val_loss: 0.1425 - val_accuracy: 0.8333\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.1426 - accuracy: 0.8000 - val_loss: 0.1416 - val_accuracy: 0.8333\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.1417 - accuracy: 0.8000 - val_loss: 0.1407 - val_accuracy: 0.8333\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.1408 - accuracy: 0.8000 - val_loss: 0.1399 - val_accuracy: 0.8333\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 0.1399 - accuracy: 0.8000 - val_loss: 0.1390 - val_accuracy: 0.8333\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 106us/sample - loss: 0.1391 - accuracy: 0.8000 - val_loss: 0.1382 - val_accuracy: 0.8333\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 105us/sample - loss: 0.1382 - accuracy: 0.8000 - val_loss: 0.1373 - val_accuracy: 0.8333\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.1374 - accuracy: 0.8000 - val_loss: 0.1364 - val_accuracy: 0.8333\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 97us/sample - loss: 0.1365 - accuracy: 0.8000 - val_loss: 0.1356 - val_accuracy: 0.8333\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.1357 - accuracy: 0.8000 - val_loss: 0.1348 - val_accuracy: 0.8333\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 112us/sample - loss: 0.1348 - accuracy: 0.8000 - val_loss: 0.1340 - val_accuracy: 0.8333\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.1340 - accuracy: 0.8000 - val_loss: 0.1332 - val_accuracy: 0.8333\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.1332 - accuracy: 0.7917 - val_loss: 0.1324 - val_accuracy: 0.8333\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.1324 - accuracy: 0.8083 - val_loss: 0.1316 - val_accuracy: 0.8000\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.1316 - accuracy: 0.7917 - val_loss: 0.1308 - val_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.1309 - accuracy: 0.7917 - val_loss: 0.1301 - val_accuracy: 0.8000\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.1301 - accuracy: 0.8083 - val_loss: 0.1293 - val_accuracy: 0.8000\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.1293 - accuracy: 0.7917 - val_loss: 0.1286 - val_accuracy: 0.8000\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.1286 - accuracy: 0.7917 - val_loss: 0.1278 - val_accuracy: 0.8000\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.1278 - accuracy: 0.8000 - val_loss: 0.1271 - val_accuracy: 0.8000\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 0.1271 - accuracy: 0.8000 - val_loss: 0.1264 - val_accuracy: 0.8000\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 113us/sample - loss: 0.1263 - accuracy: 0.8000 - val_loss: 0.1257 - val_accuracy: 0.8000\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.1256 - accuracy: 0.8000 - val_loss: 0.1250 - val_accuracy: 0.8000\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 107us/sample - loss: 0.1249 - accuracy: 0.8000 - val_loss: 0.1244 - val_accuracy: 0.8000\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.1242 - accuracy: 0.8000 - val_loss: 0.1237 - val_accuracy: 0.8000\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 106us/sample - loss: 0.1235 - accuracy: 0.8083 - val_loss: 0.1230 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0961e7b128>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s.fit(x=X_train, y=t_train, validation_data=(X_test, t_test), epochs=100, callbacks=[tensorboard_callback_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6012 (pid 6871), started 0:28:23 ago. (Use '!kill 6871' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6012\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0961a59eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./logdir_sigmoid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train values\n",
      "[[39  0  0]\n",
      " [ 0 17 23]\n",
      " [ 0  0 41]]\n",
      "for test values\n",
      "[[11  0  0]\n",
      " [ 0  4  6]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_s.predict(X_train)\n",
    "y_pred_num_val = np.argmax(y_pred, axis=1)\n",
    "t_val = np.argmax(t_train, axis=1)\n",
    "print('for train values')\n",
    "print(confusion_matrix(t_val, y_pred_num_val))\n",
    "\n",
    "y_pred = model_s.predict(X_test)\n",
    "y_pred_num_val = np.argmax(y_pred, axis=1)\n",
    "t_val = np.argmax(t_test, axis=1)\n",
    "print('for test values')\n",
    "print(confusion_matrix(t_val, y_pred_num_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, input_dim=4, activation=tf.keras.layers.Activation('sigmoid')))\n",
    "model.add(tf.keras.layers.Dense(3, activation=tf.keras.layers.Activation('softmax')))\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logdir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.RMSprop(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0923 13:57:36.451481 139682939295552 summary_ops_v2.py:1110] Model failed to serialize as JSON. Ignoring... 'Activation' object has no attribute '__name__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s 3ms/sample - loss: 1.1072 - accuracy: 0.1917 - val_loss: 1.0846 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 152us/sample - loss: 1.0872 - accuracy: 0.2583 - val_loss: 1.0704 - val_accuracy: 0.4333\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 173us/sample - loss: 1.0736 - accuracy: 0.3083 - val_loss: 1.0586 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 1.0611 - accuracy: 0.3583 - val_loss: 1.0476 - val_accuracy: 0.5333\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 84us/sample - loss: 1.0501 - accuracy: 0.3917 - val_loss: 1.0374 - val_accuracy: 0.5667\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 1.0389 - accuracy: 0.4167 - val_loss: 1.0277 - val_accuracy: 0.5667\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 104us/sample - loss: 1.0293 - accuracy: 0.5000 - val_loss: 1.0180 - val_accuracy: 0.5333\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 1.0198 - accuracy: 0.5167 - val_loss: 1.0087 - val_accuracy: 0.6000\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 1.0097 - accuracy: 0.5417 - val_loss: 0.9997 - val_accuracy: 0.6000\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 1.0003 - accuracy: 0.5667 - val_loss: 0.9908 - val_accuracy: 0.6000\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.9909 - accuracy: 0.5917 - val_loss: 0.9817 - val_accuracy: 0.6333\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 98us/sample - loss: 0.9815 - accuracy: 0.6333 - val_loss: 0.9728 - val_accuracy: 0.7000\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.9722 - accuracy: 0.6417 - val_loss: 0.9641 - val_accuracy: 0.7333\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 119us/sample - loss: 0.9634 - accuracy: 0.6667 - val_loss: 0.9555 - val_accuracy: 0.7667\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.9546 - accuracy: 0.6917 - val_loss: 0.9471 - val_accuracy: 0.7667\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 147us/sample - loss: 0.9459 - accuracy: 0.7083 - val_loss: 0.9385 - val_accuracy: 0.7667\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.9372 - accuracy: 0.7167 - val_loss: 0.9306 - val_accuracy: 0.7667\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 121us/sample - loss: 0.9285 - accuracy: 0.7333 - val_loss: 0.9224 - val_accuracy: 0.7667\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.9199 - accuracy: 0.7500 - val_loss: 0.9142 - val_accuracy: 0.7667\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 74us/sample - loss: 0.9114 - accuracy: 0.7667 - val_loss: 0.9061 - val_accuracy: 0.7667\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 140us/sample - loss: 0.9029 - accuracy: 0.7583 - val_loss: 0.8981 - val_accuracy: 0.7667\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 93us/sample - loss: 0.8948 - accuracy: 0.7667 - val_loss: 0.8901 - val_accuracy: 0.8000\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 64us/sample - loss: 0.8866 - accuracy: 0.7750 - val_loss: 0.8821 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.8789 - accuracy: 0.7750 - val_loss: 0.8745 - val_accuracy: 0.8333\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 92us/sample - loss: 0.8707 - accuracy: 0.7833 - val_loss: 0.8671 - val_accuracy: 0.8333\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.8638 - accuracy: 0.7917 - val_loss: 0.8601 - val_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.8556 - accuracy: 0.7917 - val_loss: 0.8529 - val_accuracy: 0.8000\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 105us/sample - loss: 0.8493 - accuracy: 0.7833 - val_loss: 0.8459 - val_accuracy: 0.8000\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 137us/sample - loss: 0.8413 - accuracy: 0.7833 - val_loss: 0.8391 - val_accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 86us/sample - loss: 0.8342 - accuracy: 0.7917 - val_loss: 0.8323 - val_accuracy: 0.8000\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.8270 - accuracy: 0.8083 - val_loss: 0.8254 - val_accuracy: 0.8000\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 88us/sample - loss: 0.8197 - accuracy: 0.8083 - val_loss: 0.8184 - val_accuracy: 0.8333\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 81us/sample - loss: 0.8129 - accuracy: 0.8000 - val_loss: 0.8113 - val_accuracy: 0.8333\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 82us/sample - loss: 0.8058 - accuracy: 0.8083 - val_loss: 0.8043 - val_accuracy: 0.8333\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.7991 - accuracy: 0.8083 - val_loss: 0.7976 - val_accuracy: 0.8333\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.7920 - accuracy: 0.8000 - val_loss: 0.7910 - val_accuracy: 0.8333\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 153us/sample - loss: 0.7852 - accuracy: 0.8083 - val_loss: 0.7846 - val_accuracy: 0.8333\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.7785 - accuracy: 0.8083 - val_loss: 0.7783 - val_accuracy: 0.8333\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 136us/sample - loss: 0.7722 - accuracy: 0.8083 - val_loss: 0.7720 - val_accuracy: 0.8333\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 106us/sample - loss: 0.7657 - accuracy: 0.8083 - val_loss: 0.7657 - val_accuracy: 0.8333\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 94us/sample - loss: 0.7592 - accuracy: 0.8167 - val_loss: 0.7594 - val_accuracy: 0.8333\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.7528 - accuracy: 0.8167 - val_loss: 0.7531 - val_accuracy: 0.8333\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 97us/sample - loss: 0.7466 - accuracy: 0.8167 - val_loss: 0.7471 - val_accuracy: 0.8333\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.7402 - accuracy: 0.8167 - val_loss: 0.7413 - val_accuracy: 0.8333\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 0.7344 - accuracy: 0.8167 - val_loss: 0.7353 - val_accuracy: 0.8333\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 91us/sample - loss: 0.7282 - accuracy: 0.8083 - val_loss: 0.7295 - val_accuracy: 0.8333\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 113us/sample - loss: 0.7226 - accuracy: 0.8167 - val_loss: 0.7237 - val_accuracy: 0.8333\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 113us/sample - loss: 0.7165 - accuracy: 0.8167 - val_loss: 0.7181 - val_accuracy: 0.8333\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 90us/sample - loss: 0.7108 - accuracy: 0.8333 - val_loss: 0.7125 - val_accuracy: 0.8333\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.7053 - accuracy: 0.8250 - val_loss: 0.7071 - val_accuracy: 0.8333\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.7000 - accuracy: 0.8250 - val_loss: 0.7019 - val_accuracy: 0.8333\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.6946 - accuracy: 0.8250 - val_loss: 0.6968 - val_accuracy: 0.8333\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 71us/sample - loss: 0.6901 - accuracy: 0.8167 - val_loss: 0.6917 - val_accuracy: 0.8333\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 126us/sample - loss: 0.6844 - accuracy: 0.8333 - val_loss: 0.6869 - val_accuracy: 0.8333\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.6791 - accuracy: 0.8333 - val_loss: 0.6819 - val_accuracy: 0.8333\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 0.6746 - accuracy: 0.8333 - val_loss: 0.6772 - val_accuracy: 0.8333\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 172us/sample - loss: 0.6696 - accuracy: 0.8250 - val_loss: 0.6724 - val_accuracy: 0.8333\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.6646 - accuracy: 0.8333 - val_loss: 0.6676 - val_accuracy: 0.8333\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 136us/sample - loss: 0.6596 - accuracy: 0.8333 - val_loss: 0.6628 - val_accuracy: 0.8333\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.6553 - accuracy: 0.8333 - val_loss: 0.6582 - val_accuracy: 0.8333\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.6506 - accuracy: 0.8250 - val_loss: 0.6537 - val_accuracy: 0.8333\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 113us/sample - loss: 0.6455 - accuracy: 0.8333 - val_loss: 0.6491 - val_accuracy: 0.8333\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.6407 - accuracy: 0.8333 - val_loss: 0.6446 - val_accuracy: 0.8333\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.6368 - accuracy: 0.8333 - val_loss: 0.6403 - val_accuracy: 0.8333\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 105us/sample - loss: 0.6318 - accuracy: 0.8333 - val_loss: 0.6356 - val_accuracy: 0.8333\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 128us/sample - loss: 0.6276 - accuracy: 0.8333 - val_loss: 0.6312 - val_accuracy: 0.8333\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6234 - accuracy: 0.8333 - val_loss: 0.6271 - val_accuracy: 0.8333\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 112us/sample - loss: 0.6192 - accuracy: 0.8333 - val_loss: 0.6229 - val_accuracy: 0.8333\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.6144 - accuracy: 0.8417 - val_loss: 0.6187 - val_accuracy: 0.8333\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 99us/sample - loss: 0.6106 - accuracy: 0.8333 - val_loss: 0.6145 - val_accuracy: 0.8333\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6062 - accuracy: 0.8417 - val_loss: 0.6105 - val_accuracy: 0.8333\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 129us/sample - loss: 0.6023 - accuracy: 0.8333 - val_loss: 0.6068 - val_accuracy: 0.8333\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 161us/sample - loss: 0.5982 - accuracy: 0.8333 - val_loss: 0.6031 - val_accuracy: 0.8333\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.5941 - accuracy: 0.8417 - val_loss: 0.5993 - val_accuracy: 0.8333\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 105us/sample - loss: 0.5903 - accuracy: 0.8417 - val_loss: 0.5954 - val_accuracy: 0.8333\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 115us/sample - loss: 0.5864 - accuracy: 0.8417 - val_loss: 0.5916 - val_accuracy: 0.8333\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 114us/sample - loss: 0.5825 - accuracy: 0.8417 - val_loss: 0.5877 - val_accuracy: 0.8333\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.5784 - accuracy: 0.8417 - val_loss: 0.5840 - val_accuracy: 0.8333\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.5747 - accuracy: 0.8333 - val_loss: 0.5803 - val_accuracy: 0.8333\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 77us/sample - loss: 0.5708 - accuracy: 0.8250 - val_loss: 0.5766 - val_accuracy: 0.8333\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.5671 - accuracy: 0.8333 - val_loss: 0.5731 - val_accuracy: 0.8333\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.5634 - accuracy: 0.8417 - val_loss: 0.5696 - val_accuracy: 0.8333\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.5599 - accuracy: 0.8333 - val_loss: 0.5659 - val_accuracy: 0.8333\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 146us/sample - loss: 0.5562 - accuracy: 0.8417 - val_loss: 0.5626 - val_accuracy: 0.8333\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 85us/sample - loss: 0.5527 - accuracy: 0.8333 - val_loss: 0.5591 - val_accuracy: 0.8333\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 103us/sample - loss: 0.5493 - accuracy: 0.8333 - val_loss: 0.5555 - val_accuracy: 0.8333\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 79us/sample - loss: 0.5457 - accuracy: 0.8500 - val_loss: 0.5524 - val_accuracy: 0.8333\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 110us/sample - loss: 0.5424 - accuracy: 0.8500 - val_loss: 0.5490 - val_accuracy: 0.8333\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 0.5389 - accuracy: 0.8500 - val_loss: 0.5458 - val_accuracy: 0.8333\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 131us/sample - loss: 0.5359 - accuracy: 0.8583 - val_loss: 0.5426 - val_accuracy: 0.8333\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 130us/sample - loss: 0.5324 - accuracy: 0.8500 - val_loss: 0.5397 - val_accuracy: 0.8333\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 170us/sample - loss: 0.5293 - accuracy: 0.8583 - val_loss: 0.5366 - val_accuracy: 0.8333\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 95us/sample - loss: 0.5269 - accuracy: 0.8500 - val_loss: 0.5337 - val_accuracy: 0.8333\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 105us/sample - loss: 0.5231 - accuracy: 0.8583 - val_loss: 0.5308 - val_accuracy: 0.8333\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 101us/sample - loss: 0.5200 - accuracy: 0.8583 - val_loss: 0.5278 - val_accuracy: 0.8333\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 132us/sample - loss: 0.5171 - accuracy: 0.8500 - val_loss: 0.5250 - val_accuracy: 0.8333\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.5143 - accuracy: 0.8500 - val_loss: 0.5221 - val_accuracy: 0.8333\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 109us/sample - loss: 0.5114 - accuracy: 0.8583 - val_loss: 0.5193 - val_accuracy: 0.8333\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 111us/sample - loss: 0.5083 - accuracy: 0.8583 - val_loss: 0.5168 - val_accuracy: 0.8333\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.5055 - accuracy: 0.8583 - val_loss: 0.5142 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f09619a8fd0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=t_train, validation_data=(X_test, t_test), epochs=100, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6011 (pid 6782), started 0:32:04 ago. (Use '!kill 6782' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6011\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0961941e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logdir='./logdir'\n",
    "# %load_ext tensorboard\n",
    "%tensorboard --logdir=./logdir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for train values\n",
      "[[39  0  0]\n",
      " [ 0 26 14]\n",
      " [ 0  3 38]]\n",
      "for test values\n",
      "[[11  0  0]\n",
      " [ 0  7  3]\n",
      " [ 0  2  7]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "y_pred_num_val = np.argmax(y_pred, axis=1)\n",
    "t_val = np.argmax(t_train, axis=1)\n",
    "print('for train values')\n",
    "print(confusion_matrix(t_val, y_pred_num_val))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_num_val = np.argmax(y_pred, axis=1)\n",
    "t_val = np.argmax(t_test, axis=1)\n",
    "print('for test values')\n",
    "print(confusion_matrix(t_val, y_pred_num_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
