{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLDisQ6gAaMQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pwyZu2ibAhO1",
    "outputId": "01503b88-86a2-4619-d029-6ba8a7632cd8"
   },
   "outputs": [],
   "source": [
    "def loadIrisData():\n",
    "    iris = load_iris()\n",
    "    X=iris['data']\n",
    "    t=iris['target']\n",
    "    print(X.shape)\n",
    "    print(t.shape)\n",
    "    return X, t\n",
    "# y, t = loadIrisData()\n",
    "# np.insert(y,0,1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48_5__HzBks_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(t_indices, N):\n",
    "    '''\n",
    "    Inputs:\n",
    "        t_indices: list of indices\n",
    "        N: total no. of classes\n",
    "    '''\n",
    "    assert N>max(t_indices), (N, max(t_indices))\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "    one_hot_encoded = np.zeros((len(t_indices), N))\n",
    "    unique_list = np.unique(t_indices)\n",
    "    print(unique_list)\n",
    "    for i in unique_list:\n",
    "        idx = np.where(t_indices==i)[0]\n",
    "#         print(idx)\n",
    "        one_hot_encoded[idx, i] = 1\n",
    "\n",
    "    t_1hot = one_hot_encoded\n",
    "    return t_1hot\n",
    "# one_hot_encoding(t, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2DsnXa89lIk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n",
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_one_hot_encoding():\n",
    "    t_1hot = one_hot_encoding([0,2], 3)\n",
    "    t_1hotTrue = np.array([[1.,0.,0.], [0.,0.,1.]])\n",
    "    assert np.all(np.isclose( t_1hot, t_1hotTrue ))\n",
    "    print('Test passed', '\\U0001F44D')\n",
    "if __name__==\"__main__\":\n",
    "    test_one_hot_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXxxA2YkB_I8"
   },
   "outputs": [],
   "source": [
    "def splitData(X,t,testFraction=0.2):\n",
    "    \"\"\"\n",
    "    Use numpy functions only\n",
    "    Inputs:\n",
    "        X: np array of shape (Nsamples, dim)\n",
    "        t: np array of len Nsamples; can be one hot vectors or labels\n",
    "        testFraction: (float) Nsamples_test = testFraction * Nsamples\n",
    "    \"\"\"\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "#     print(np.floor(len(X)*testFraction))\n",
    "    arr = np.arange(len(X))\n",
    "    np.random.shuffle(arr)\n",
    "    X = X[arr]\n",
    "    t = t[arr]\n",
    "    \n",
    "    X_test = X[:int(np.floor(testFraction*len(X))), :]\n",
    "    t_test = t[:int(np.floor(testFraction*len(t))), :]\n",
    "    \n",
    "    X_train = X[int(np.floor(testFraction*len(X))):, :]\n",
    "    t_train = t[int(np.floor(testFraction*len(t))):, :]\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_splitData():\n",
    "    X = np.random.random((5,2))\n",
    "    t1hot = one_hot_encoding([1,0,2,1,2],3)\n",
    "    X_train, t1hot_train, X_test, t1hot_test = splitData(X,t1hot,.2)\n",
    "    assert X_train.shape==(4,2), [\"X_train.shape\", X_train.shape]\n",
    "    assert X_test.shape==(1,2), [\"X_test.shape\", X_test.shape]\n",
    "    print('Test passed', '\\U0001F44D')    \n",
    "if __name__==\"__main__\":\n",
    "    test_splitData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OK2lZ6ZpCjAg"
   },
   "outputs": [],
   "source": [
    "### Normalize data to be of zero mean and unit variance\n",
    "def normalizeX(X_train, X_test):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X_train: np array 2d\n",
    "        X_test: np array 2d\n",
    "    Outputs:\n",
    "        Normalized np arrays 2d\n",
    "    '''\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "    \n",
    "    X_test_normalized = (X_test-np.float32([np.mean(X_test, axis=0)]))\n",
    "    X_test_normalized = X_test_normalized/np.float32([np.std(X_test_normalized, axis=0)])\n",
    "    X_train_normalized = (X_train-np.float32([np.mean(X_train, axis=0)]))\n",
    "    X_train_normalized = X_train_normalized/np.float32([np.std(X_train_normalized, axis=0)])\n",
    "\n",
    "    return X_train_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_normalizeX():\n",
    "    X_train = np.array([[1,1,0],[2,2,1]])\n",
    "    X_test = np.array([[1,1,0],[3,3,2]])\n",
    "    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)\n",
    "    a = np.array([[-1.,-1.,-1.], [ 1., 1., 1.]])\n",
    "    b = np.array([[-1.,-1.,-1.], [ 1., 1., 1.]])\n",
    "    assert np.all(np.isclose( X_train_normalized, a )), a\n",
    "    assert np.all(np.isclose( X_test_normalized, b )), b\n",
    "    print('Test passed', '\\U0001F44D')    \n",
    "if __name__==\"__main__\":\n",
    "    test_normalizeX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJ_OSEoQLEuc"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Input:\n",
    "        x: numpy array of any shape\n",
    "    Output:\n",
    "        y: numpy array of same shape as x\n",
    "    '''\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 1 MARKS\n",
    "    \n",
    "    y = 1. / (1. + np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_sigmoid():\n",
    "    x = np.array([np.log(4),np.log(0.25),0])\n",
    "    y = sigmoid(x)\n",
    "    assert np.all(np.isclose( y, np.array([0.8, 0.2, 0.5]) )), y\n",
    "    print('Test passed', '\\U0001F44D')    \n",
    "if __name__==\"__main__\":\n",
    "    test_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Input:\n",
    "        x: numpy array of any shape\n",
    "    Output:\n",
    "        y: numpy array of same shape as x\n",
    "    '''\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 1 MARKS\n",
    "    \n",
    "    y = np.exp(x)/(np.sum(np.exp(x)))\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_softmax():\n",
    "    x = np.array([np.log(2),np.log(7),0])\n",
    "    y = softmax(x)\n",
    "    assert np.all(np.isclose( y, np.array([0.2, 0.7, 0.1]) )), y\n",
    "    print('Test passed', '\\U0001F44D')    \n",
    "if __name__==\"__main__\":\n",
    "    test_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iwi4QwxlOAOR"
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    '''\n",
    "    Input:\n",
    "        x: numpy array of any shape; it is sigmoid layer's output\n",
    "    Output:\n",
    "        y: numpy array of same shape as x; it is the derivative of sigmoid\n",
    "    '''\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 1 MARKS\n",
    "    \n",
    "    y = np.exp(-x)/np.square((1. + np.exp(-x)))\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        '''   \n",
    "        Input:\n",
    "            ni: int, size of input layer\n",
    "            nh: int, size of hidden layer\n",
    "            no: int, size of output layer\n",
    "        Action:\n",
    "            Creates instance variables\n",
    "        NOTE: We do not use bias explicitly here. Input x can have the first element 1 to have a bias term.\n",
    "        '''\n",
    "        self.ni = ni\n",
    "        self.nh = nh\n",
    "        self.no = no\n",
    "        self.weights1 = []\n",
    "        self.weights2 = []\n",
    "        return\n",
    "    \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Action:\n",
    "            Randomly initialize weights1 and weights2 with proper size random np arrays\n",
    "        '''\n",
    "\n",
    "        ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "#         self.weights1 = np.reshape(np.random.rand((ni+1)*(nh+1)), ((nh+1), (ni+1)))\n",
    "#         self.weights2 = np.reshape(np.random.rand((no)*(nh+1)), ((no), (nh+1)))\n",
    "        self.weights1 =2*np.random.rand(self.nh,self.ni+1)-1\n",
    "        self.weights2 =2*np.random.rand(self.no,self.nh+1)-1\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.insert(x,0,1,axis=0) # inserts a row of 1s. This is for the bias\n",
    "        h1 = self.weights1.dot(x)\n",
    "        v1 = sigmoid(h1)\n",
    "        v1 = np.insert(v1,0,1,axis=0) # inserts a row of 1s. This is for the bias\n",
    "        h2 = self.weights2.dot(v1)\n",
    "        v2 = softmax(h2)\n",
    "        return v2\n",
    "\n",
    "    def backprop(self,x,y,eta):\n",
    "        '''\n",
    "        # application of the chain rule to find derivative of the categorical cross entropy loss function with respect to weights2 and weights1\n",
    "        Input:\n",
    "            x: numpy array of shape (ni,1)\n",
    "            y: numpy array of shape (no,1)\n",
    "            eta: learning rate\n",
    "        Action:\n",
    "            # Finding the derivatives\n",
    "            del_weights2: np array that stores the derivative of the loss function with respect to weights2\n",
    "            del_weights1: np array that stores the derivative of the loss function with respect to weights1\n",
    "\n",
    "            # Update the weights with the derivative of the categorical cross entropy loss function\n",
    "              weights1 += eta*del_weights1\n",
    "              weights2 += eta*del_weights2\n",
    "        ''' \n",
    "\n",
    "        ### WRITE YOUR CODE HERE - 5 MARKS\n",
    "        x1 = np.insert(x,0,1,axis=0) # inserts a row of 1s. This is for the bias\n",
    "        h1 = self.weights1.dot(x1)\n",
    "        v1 = sigmoid(h1)\n",
    "        \n",
    "        v = copy.deepcopy(v1)\n",
    "        \n",
    "        v1 = np.insert(v1,0,1,axis=0) # inserts a row of 1s. This is for the bias\n",
    "        h2 = self.weights2.dot(v1)\n",
    "        v2 = softmax(h2)\n",
    "        \n",
    "        \n",
    "        v2=v2.reshape((len(v2),1))\n",
    "        v=v.reshape((len(v),1))\n",
    "        y=y.reshape((len(y),1))\n",
    "        \n",
    "        #get bias\n",
    "        b1 = self.weights1[:,:1]\n",
    "        b2 = self.weights2[:,:1]\n",
    "        \n",
    "        #get rest weights\n",
    "        w1 = self.weights1[:,1:]\n",
    "        w2 = self.weights2[:,1:]\n",
    "        \n",
    "        #get gradients\n",
    "        del_weights2 = (y-v2).dot(v.T)\n",
    "        del_weights1 = sigmoid_derivative(v)*(w2.T.dot(y-v2))\n",
    "        \n",
    "#         print(del_weights1, del_weights2)\n",
    "#         print(w1.shape, del_weights1.shape, x.shape)\n",
    "        #update weights\n",
    "        w1 = w1 + eta*del_weights1*(x.T)\n",
    "        w2 = w2 + eta*del_weights2\n",
    "        \n",
    "        #update biases\n",
    "        b2 = b2 + eta*(y-v2)\n",
    "        b1 = b1 + eta*del_weights1\n",
    "        \n",
    "        #add biases to weights array\n",
    "        self.weights1 = np.append(b1,w1,axis=1)\n",
    "        self.weights2 = np.append(b2,w2,axis=1)\n",
    "        \n",
    "        #return loss\n",
    "        loss = (y*np.log(v2)).sum()\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def fit(self, X, t, eta, epochs):\n",
    "        '''\n",
    "        input:\n",
    "            X: training input data \n",
    "            t: training targets\n",
    "            eta: learning rate\n",
    "            epochs: number of epochs\n",
    "        Action:\n",
    "            train the weights\n",
    "        '''\n",
    "\n",
    "        ### WRITE YOUR CODE HERE - 5 MARKS\n",
    "        loss = np.zeros(epochs)\n",
    "        \n",
    "        #init weights for code\n",
    "        self.init_weights()\n",
    "        \n",
    "        #number of training exaples\n",
    "        n_examples = len(X)\n",
    "        for i in range(n_examples):\n",
    "            for j in range(epochs):\n",
    "                x = X[i,:].T\n",
    "                y = t[i,:].T\n",
    "                loss_val = self.backprop(x,y,eta)\n",
    "                loss[j] = loss[j]-loss_val\n",
    "        \n",
    "        loss=loss/(len(X))\n",
    "        return self.weights2, loss\n",
    "        \n",
    "    def predict_label(self,x):    \n",
    "        '''\n",
    "        Output:\n",
    "            y: np array of index\n",
    "        '''\n",
    "\n",
    "        ### WRITE YOUR CODE HERE - 1 MARKS\n",
    "        y = np.zeros((len(x),1))\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            v2 = self.predict(x[i])\n",
    "            y[i][0] = np.argmax(v2)\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "fmR8F2JIFwqm",
    "outputId": "fa868689-92c8-4a43-882f-3321cde1a301"
   },
   "outputs": [],
   "source": [
    "### Lastly, report the accuracy of your model and print the Confusion Matrix\n",
    "#printing the confusion matrix\n",
    "def getCM(y,t):\n",
    "    '''\n",
    "    Inputs:\n",
    "        y: estimated labels np array (Nsample,1)\n",
    "        t: targets np array (Nsamples,1)\n",
    "    Outputs:\n",
    "        CM : np array of confusion matrix\n",
    "    '''\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 3 MARKS\n",
    "    conf_M = np.zeros((3,3))\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        conf_M[int(t[i][0])][int(y[i][0])] = conf_M[int(t[i][0])][int(y[i][0])] + 1\n",
    "        \n",
    "    return conf_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_actual_value(t):\n",
    "    y = np.zeros((len(t),1))\n",
    "    for i in range(len(t)):\n",
    "        y[i][0] = np.argmax(t[i])\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments\n",
    "Use the above functions to carry out the experiment:\n",
    "- load iris data and prepare it for NN\n",
    "- split randomly into 20% test data\n",
    "- create a NN with 1 hidden layer\n",
    "- train the network with training data\n",
    "- Plot loss w.r.t. number of epochs\n",
    "- Print confusion matrix on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hewsBv12weZ2",
    "outputId": "52407420-e7e5-4ca6-952b-6448ffe4b101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "[0 1 2]\n",
      "(120, 3) (120, 4)\n",
      "[[10.  0.  0.]\n",
      " [ 0.  6.  4.]\n",
      " [ 0.  0. 10.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def experiment():\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 10 MARKS\n",
    "    X,y=loadIrisData()\n",
    "    \n",
    "    y_hot = one_hot_encoding(y,3)\n",
    "    \n",
    "#     print(y_hot)\n",
    "    \n",
    "    X_train, y_hot_train, X_test, y_hot_test = splitData(X,y_hot,0.2)\n",
    "    \n",
    "    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)\n",
    "    \n",
    "#     print(y_hot_train.shape, X_train_normalized.shape)\n",
    "    no, nh, ni = 3, 5, 4\n",
    "    \n",
    "    NN = NeuralNetwork(ni,nh,no)\n",
    "    \n",
    "    \n",
    "    weights2, Loss=NN.fit(X_train_normalized, y_hot_train,0.01,10000)\n",
    "    \n",
    "    labels = np.empty([X_test.shape[0],1])\n",
    "    \n",
    "    t = np.empty([X_test.shape[0],1])\n",
    "    \n",
    "    labels = NN.predict_label(X_test_normalized)\n",
    "    \n",
    "    t = predict_actual_value(y_hot_test)\n",
    "    \n",
    "    print(getCM(labels,t))\n",
    "    plt.plot(Loss)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BP_Iris.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
