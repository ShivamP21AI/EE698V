{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLDisQ6gAaMQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pwyZu2ibAhO1",
    "outputId": "01503b88-86a2-4619-d029-6ba8a7632cd8"
   },
   "outputs": [],
   "source": [
    "def loadIrisData():\n",
    "    iris = load_iris()\n",
    "    X=iris['data']\n",
    "    t=iris['target']\n",
    "    print(X.shape)\n",
    "    print(t.shape)\n",
    "    return X, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48_5__HzBks_"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(t_indices, N):\n",
    "    '''\n",
    "    Inputs:\n",
    "        t_indices: list of indices\n",
    "        N: total no. of classes\n",
    "    '''\n",
    "    assert N>max(t_indices), (N, max(t_indices))\n",
    "    t_1hot=np.empty([len(t_indices),N])\n",
    "    for t in range(len(t_indices)):\n",
    "        l=np.zeros(N)\n",
    "        l[t_indices[t]]=1\n",
    "        t_1hot[t]=l\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "\n",
    "    return t_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2DsnXa89lIk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_one_hot_encoding():\n",
    "    t_1hot = one_hot_encoding([0,2], 3)\n",
    "    t_1hotTrue = np.array([[1.,0.,0.], [0.,0.,1.]])\n",
    "    assert np.all(np.isclose( t_1hot, t_1hotTrue ))\n",
    "    print('Test passed', '\\U0001F44D')\n",
    "if __name__==\"__main__\":\n",
    "    test_one_hot_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXxxA2YkB_I8"
   },
   "outputs": [],
   "source": [
    "def splitData(X,t,testFraction=0.2):\n",
    "    \"\"\"\n",
    "    Use numpy functions only\n",
    "    Inputs:\n",
    "        X: np array of shape (Nsamples, dim)\n",
    "        t: np array of len Nsamples; can be one hot vectors or labels\n",
    "        testFraction: (float) Nsamples_test = testFraction * Nsamples\n",
    "    \"\"\"\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "    index = np.arange(len(X))\n",
    "    np.random.shuffle(index)\n",
    "    Nsamples = X.shape[0]\n",
    "    Nsamples_test = int(testFraction*Nsamples)\n",
    "    index_test = index[:Nsamples_test]\n",
    "    index_train = index[Nsamples_test:]\n",
    "    X_train = np.zeros((len(index_train),len(X[0])))\n",
    "    X_test = np.zeros((len(index_test),len(X[0])))\n",
    "    t_train = np.zeros((len(index_train),len(t[0])))\n",
    "    t_test = np.zeros((len(index_test),len(t[0])))\n",
    "    for i in range(len(index_test)):\n",
    "        X_test[i] = X[index_test[i]]\n",
    "        t_test[i] = t[index_test[i]]\n",
    "    \n",
    "    for i in range(len(index_train)):\n",
    "        X_train[i] = X[index_train[i]]\n",
    "        t_train[i] = t[index_train[i]]\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_splitData():\n",
    "    X = np.random.random((5,2))\n",
    "    t1hot = one_hot_encoding([1,0,2,1,2],3)\n",
    "    X_train, t1hot_train, X_test, t1hot_test = splitData(X,t1hot,.2)\n",
    "    assert X_train.shape==(4,2), [\"X_train.shape\", X_train.shape]\n",
    "    assert X_test.shape==(1,2), [\"X_test.shape\", X_test.shape]\n",
    "    print('Test passed', '\\U0001F44D')    \n",
    "if __name__==\"__main__\":\n",
    "    test_splitData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OK2lZ6ZpCjAg"
   },
   "outputs": [],
   "source": [
    "### Normalize data to be of zero mean and unit variance\n",
    "def normalizeX(X_train, X_test):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X_train: np array 2d\n",
    "        X_test: np array 2d\n",
    "    Outputs:\n",
    "        Normalized np arrays 2d\n",
    "    '''\n",
    "    mean_test = np.mean(X_test,axis = 0)\n",
    "    mean_train = np.mean(X_train, axis= 0)\n",
    "    std_test = np.std(X_test,axis=0)\n",
    "    std_train = np.std(X_train,axis=0)\n",
    "    X_train_normalized = (X_train-mean_train)/std_train\n",
    "    X_test_normalized = (X_test-mean_train)/std_train\n",
    "    ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "\n",
    "    return X_train_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_normalizeX():\n",
    "    X_train = np.array([[1,1,0],[2,2,1]])\n",
    "    X_test = np.array([[1,1,0],[3,3,2]])\n",
    "    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)\n",
    "    a = np.array([[-1.,-1.,-1.], [ 1., 1., 1.]])\n",
    "    b = np.array([[-1.,-1.,-1.], [ 3.,3.,3.]])\n",
    "    assert np.all(np.isclose( X_train_normalized, a )), a\n",
    "    assert np.all(np.isclose( X_test_normalized, b )), b\n",
    "    print('Test passed', '\\U0001F44D')    \n",
    "if __name__==\"__main__\":\n",
    "    test_normalizeX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJ_OSEoQLEuc"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Input:\n",
    "        x: numpy array of any shape\n",
    "    Output:\n",
    "        y: numpy array of same shape as x\n",
    "    '''\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    ### WRITE YOUR CODE HERE - 1 MARKS\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_sigmoid():\n",
    "    x = np.array([np.log(4),np.log(0.25),0])\n",
    "    y = sigmoid(x)\n",
    "    assert np.all(np.isclose( y, np.array([0.8, 0.2, 0.5]) )), y\n",
    "    print('Test passed', '\\U0001F44D')    \n",
    "if __name__==\"__main__\":\n",
    "    test_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Input:\n",
    "        x: numpy array of any shape\n",
    "    Output:\n",
    "        y: numpy array of same shape as x\n",
    "    '''\n",
    "    div = sum(np.exp(x))\n",
    "    y = np.exp(x)/div\n",
    "    ### WRITE YOUR CODE HERE - 1 MARKS\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "def test_softmax():\n",
    "    x = np.array([np.log(2),np.log(7),0])\n",
    "    y = softmax(x)\n",
    "    assert np.all(np.isclose( y, np.array([0.2, 0.7, 0.1]) )), y\n",
    "    print('Test passed', '\\U0001F44D')    \n",
    "if __name__==\"__main__\":\n",
    "    test_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iwi4QwxlOAOR"
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    '''\n",
    "    Input:\n",
    "        x: numpy array of any shape; it is sigmoid layer's output\n",
    "    Output:\n",
    "        y: numpy array of same shape as x; it is the derivative of sigmoid\n",
    "    '''\n",
    "    y = (x)*(1-x)\n",
    "    ### WRITE YOUR CODE HERE - 1 MARKS\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        '''   \n",
    "        Input:\n",
    "            ni: int, size of input layer\n",
    "            nh: int, size of hidden layer\n",
    "            no: int, size of output layer\n",
    "        Action:\n",
    "            Creates instance variables\n",
    "        NOTE: We do not use bias explicitly here. Input x can have the first element 1 to have a bias term.\n",
    "        '''\n",
    "        self.ni = ni\n",
    "        self.nh = nh\n",
    "        self.no = no\n",
    "        self.weights1 = []\n",
    "        self.weights2 = []\n",
    "        return\n",
    "    \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Action:\n",
    "            Randomly initialize weights1 and weights2 with proper size random np arrays\n",
    "        '''\n",
    "        self.weights1 =2*np.random.rand(self.nh,self.ni+1)-1\n",
    "        self.weights2 =2*np.random.rand(self.no,self.nh+1)-1\n",
    "         ### WRITE YOUR CODE HERE - 2 MARKS\n",
    "\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.insert(x,0,1,axis=0) # inserts a row of 1s. This is for the bias\n",
    "        h1 = self.weights1.dot(x)\n",
    "        v1 = sigmoid(h1)\n",
    "        v = v1\n",
    "        v1 = np.insert(v1,0,1,axis=0) # inserts a row of 1s. This is for the bias\n",
    "        h2 = self.weights2.dot(v1)\n",
    "        v2 = softmax(h2)\n",
    "        return v2\n",
    "\n",
    "    def backprop(self,x,y,eta):\n",
    "        '''\n",
    "        # application of the chain rule to find derivative of the categorical cross entropy loss function with respect to weights2 and weights1\n",
    "        Input:\n",
    "            x: numpy array of shape (ni,1)\n",
    "            y: numpy array of shape (no,1)\n",
    "            eta: learning rate\n",
    "        Action:\n",
    "            # Finding the derivatives\n",
    "            del_weights2: np array that stores the derivative of the loss function with respect to weights2\n",
    "            del_weights1: np array that stores the derivative of the loss function with respect to weights1\n",
    "\n",
    "            # Update the weights with the derivative of the categorical cross entropy loss function\n",
    "              weights1 += eta*del_weights1\n",
    "              weights2 += eta*del_weights2\n",
    "        ''' \n",
    "        x1 = np.insert(x,0,1,axis=0) # inserts a row of 1s. This is for the bias\n",
    "        h1 = self.weights1.dot(x1)\n",
    "        v1 = sigmoid(h1)\n",
    "        v = v1\n",
    "        v1 = np.insert(v1,0,1,axis=0) # inserts a row of 1s. This is for the bias\n",
    "        h2 = self.weights2.dot(v1)\n",
    "        v2 = softmax(h2)\n",
    "        \n",
    "        v2=v2.reshape((3,1))\n",
    "        v=v.reshape((5,1))\n",
    "        y=y.reshape((3,1))\n",
    "        bias1 = self.weights1[:,:1]\n",
    "        bias2 = self.weights2[:,:1]\n",
    "        w1 = self.weights1[:,1:]\n",
    "        w2 = self.weights2[:,1:]\n",
    "        del_weights2 = (y-v2).dot(v.T)\n",
    "        del_weights1 = sigmoid_derivative(v)*(w2.T.dot(y-v2))\n",
    "        \n",
    "        bias2 = bias2 + eta*(y-v2)\n",
    "        bias1 = bias1 + eta*del_weights1\n",
    "        \n",
    "        w1 = w1 + eta*del_weights1*(x.T)\n",
    "        w2 = w2 + eta*del_weights2\n",
    "        \n",
    "        self.weights1 = np.append(bias1,w1,axis=1)\n",
    "        self.weights2 = np.append(bias2,w2,axis=1)\n",
    "        \n",
    "        loss_val = (y*np.log(v2)).sum()\n",
    "        return loss_val\n",
    "        \n",
    "        ### WRITE YOUR CODE HERE - 5 MARKS\n",
    "\n",
    "\n",
    "    def fit(self, X, t, eta, epochs):\n",
    "        '''\n",
    "        input:\n",
    "            X: training input data \n",
    "            t: training targets\n",
    "            eta: learning rate\n",
    "            epochs: number of epochs\n",
    "        Action:\n",
    "            train the weights\n",
    "        '''\n",
    "        loss = np.zeros(epochs)\n",
    "        self.init_weights()\n",
    "        number_of_samples = X.shape[0]\n",
    "        for i in range(number_of_samples):\n",
    "            for j in range(epochs):\n",
    "                x = X[i,:].T\n",
    "                y = t[i,:].T\n",
    "                loss_val = self.backprop(x,y,eta)\n",
    "                loss[j] = loss[j]-loss_val\n",
    "        ### WRITE YOUR CODE HERE - 5 MARKS\n",
    "        \n",
    "        loss=loss/(len(X))\n",
    "        return self.weights2, loss\n",
    "        \n",
    "    def predict_label(self,x):    \n",
    "        '''\n",
    "        Output:\n",
    "            y: np array of index\n",
    "        '''\n",
    "        y = np.zeros((len(x),1))\n",
    "        for i in range(len(x)):\n",
    "            v2 = self.predict(x[i])\n",
    "            y[i][0] = np.argmax(v2)\n",
    "        ### WRITE YOUR CODE HERE - 1 MARKS\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "fmR8F2JIFwqm",
    "outputId": "fa868689-92c8-4a43-882f-3321cde1a301"
   },
   "outputs": [],
   "source": [
    "### Lastly, report the accuracy of your model and print the Confusion Matrix\n",
    "#printing the confusion matrix\n",
    "def getCM(y,t):\n",
    "    '''\n",
    "    Inputs:\n",
    "        y: estimated labels np array (Nsample,1)\n",
    "        t: targets np array (Nsamples,1)\n",
    "    Outputs:\n",
    "        CM : np array of confusion matrix\n",
    "    '''\n",
    "    CM = np.zeros((3,3))\n",
    "    for i in range(len(y)):\n",
    "        CM[int(t[i][0])][int(y[i][0])] = CM[int(t[i][0])][int(y[i][0])] + 1\n",
    "    ### WRITE YOUR CODE HERE - 3 MARKS\n",
    "\n",
    "    return CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_act(t):\n",
    "    y = np.zeros((len(t),1))\n",
    "    for i in range(len(t)):\n",
    "        y[i][0] = np.argmax(t[i])\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments\n",
    "Use the above functions to carry out the experiment:\n",
    "- load iris data and prepare it for NN\n",
    "- split randomly into 20% test data\n",
    "- create a NN with 1 hidden layer\n",
    "- train the network with training data\n",
    "- Plot loss w.r.t. number of epochs\n",
    "- Print confusion matrix on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hewsBv12weZ2",
    "outputId": "52407420-e7e5-4ca6-952b-6448ffe4b101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "[[10.  0.  0.]\n",
      " [ 0.  9.  0.]\n",
      " [ 0.  0. 11.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def experiment():\n",
    "\n",
    "    ### WRITE YOUR CODE HERE - 10 MARKS\n",
    "    X,y=loadIrisData()\n",
    "    y_hot = one_hot_encoding(y,3)\n",
    "    X_train, y_hot_train, X_test, y_hot_test = splitData(X,y_hot,0.2)\n",
    "    X_train_normalized, X_test_normalized = normalizeX(X_train, X_test)\n",
    "    ni = X.shape[1]\n",
    "    no = 3\n",
    "    nh = 5\n",
    "    ni = 4\n",
    "    NN = NeuralNetwork(ni,nh,no)\n",
    "    weights2, Loss=NN.fit(X_train_normalized, y_hot_train,0.01,10000)\n",
    "    labels = np.empty([X_test.shape[0],1])\n",
    "    t = np.empty([X_test.shape[0],1])\n",
    "    labels = NN.predict_label(X_test_normalized)\n",
    "    t = predict_act(y_hot_test)\n",
    "    print(getCM(labels,t))\n",
    "    plt.plot(Loss)\n",
    "if __name__==\"__main__\":\n",
    "    experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BP_Iris.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
